"""
ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ë° ìµœì  ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
Mac(MPS), CUDA GPU, CPUë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ì ì ˆí•œ ì„¤ì • íŒŒì¼ ì„ íƒ
"""

import torch
import platform
import subprocess
import sys
from pathlib import Path


def detect_device():
    """
    ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€

    Returns:
        device_type: 'mps', 'cuda', 'cpu'
        device_info: ë””ë°”ì´ìŠ¤ ìƒì„¸ ì •ë³´
    """
    device_info = {}

    # ì‹œìŠ¤í…œ ì •ë³´
    system = platform.system()
    device_info['system'] = system
    device_info['platform'] = platform.platform()
    device_info['processor'] = platform.processor()

    # Mac (Apple Silicon) ì²´í¬
    if system == 'Darwin':  # macOS
        try:
            # Apple Silicon í™•ì¸
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'],
                                  capture_output=True, text=True)
            cpu_info = result.stdout.strip()
            device_info['cpu'] = cpu_info

            if 'Apple' in cpu_info and torch.backends.mps.is_available():
                device_info['device'] = 'mps'
                device_info['type'] = 'Apple Silicon GPU (Metal Performance Shaders)'
                return 'mps', device_info
        except:
            pass

    # CUDA GPU ì²´í¬
    if torch.cuda.is_available():
        device_info['device'] = 'cuda'
        device_info['cuda_version'] = torch.version.cuda
        device_info['gpu_count'] = torch.cuda.device_count()
        device_info['gpu_names'] = []
        device_info['gpu_memory'] = []

        for i in range(torch.cuda.device_count()):
            device_info['gpu_names'].append(torch.cuda.get_device_name(i))
            # GPU ë©”ëª¨ë¦¬ ì •ë³´ (GB ë‹¨ìœ„)
            total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            device_info['gpu_memory'].append(f"{total_memory:.2f} GB")

        device_info['type'] = 'NVIDIA CUDA GPU'
        return 'cuda', device_info

    # CPU í´ë°±
    device_info['device'] = 'cpu'
    device_info['type'] = 'CPU Only'
    device_info['cpu_count'] = torch.get_num_threads()

    return 'cpu', device_info


def get_recommended_settings(device_type, device_info):
    """
    ë””ë°”ì´ìŠ¤ì— ë”°ë¥¸ ê¶Œì¥ ì„¤ì • ë°˜í™˜

    Args:
        device_type: 'mps', 'cuda', 'cpu'
        device_info: ë””ë°”ì´ìŠ¤ ì •ë³´

    Returns:
        settings: ê¶Œì¥ ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    settings = {}

    if device_type == 'mps':
        # Mac MPS ì„¤ì •
        settings['config_file'] = 'hyp_mac.yaml'
        settings['device'] = 'mps'
        settings['batch_size'] = 8
        settings['workers'] = 4
        settings['amp'] = False
        settings['cache'] = True
        settings['notes'] = 'Mac M1/M2/M3 ìµœì í™” ì„¤ì •'

    elif device_type == 'cuda':
        # CUDA GPU ì„¤ì •
        settings['config_file'] = 'hyp_cuda.yaml'
        settings['device'] = '0' if device_info['gpu_count'] == 1 else '0,1'

        # GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ë°°ì¹˜ í¬ê¸° ì¡°ì •
        if device_info.get('gpu_memory'):
            memory = float(device_info['gpu_memory'][0].split()[0])
            if memory < 8:
                settings['batch_size'] = 8
            elif memory < 12:
                settings['batch_size'] = 16
            elif memory < 16:
                settings['batch_size'] = 32
            elif memory < 24:
                settings['batch_size'] = 64
            else:
                settings['batch_size'] = 128
        else:
            settings['batch_size'] = 16

        settings['workers'] = 16
        settings['amp'] = True
        settings['cache'] = False
        settings['notes'] = f"NVIDIA GPU ìµœì í™” ({device_info.get('gpu_names', ['Unknown'])[0]})"

    else:
        # CPU ì„¤ì •
        settings['config_file'] = 'hyp.yaml'
        settings['device'] = 'cpu'
        settings['batch_size'] = 4
        settings['workers'] = 2
        settings['amp'] = False
        settings['cache'] = True
        settings['notes'] = 'CPU ì „ìš© ì„¤ì • (ëŠë¦¼ ì£¼ì˜)'

    return settings


def print_device_info(device_type, device_info, settings):
    """
    ë””ë°”ì´ìŠ¤ ì •ë³´ì™€ ê¶Œì¥ ì„¤ì • ì¶œë ¥
    """
    print("=" * 60)
    print("            ë””ë°”ì´ìŠ¤ ê°ì§€ ê²°ê³¼")
    print("=" * 60)

    print(f"ì‹œìŠ¤í…œ: {device_info.get('system', 'Unknown')}")
    print(f"í”„ë¡œì„¸ì„œ: {device_info.get('processor', 'Unknown')}")

    if device_type == 'mps':
        print(f"ë””ë°”ì´ìŠ¤: {device_info['type']}")
        print(f"CPU: {device_info.get('cpu', 'Unknown')}")

    elif device_type == 'cuda':
        print(f"ë””ë°”ì´ìŠ¤: {device_info['type']}")
        print(f"CUDA ë²„ì „: {device_info.get('cuda_version', 'Unknown')}")
        print(f"GPU ê°œìˆ˜: {device_info.get('gpu_count', 0)}")
        for i, (name, mem) in enumerate(zip(device_info.get('gpu_names', []),
                                           device_info.get('gpu_memory', []))):
            print(f"  GPU {i}: {name} ({mem})")

    else:
        print(f"ë””ë°”ì´ìŠ¤: {device_info['type']}")
        print(f"CPU ìŠ¤ë ˆë“œ: {device_info.get('cpu_count', 'Unknown')}")

    print("\n" + "=" * 60)
    print("            ê¶Œì¥ ì„¤ì •")
    print("=" * 60)
    print(f"ì„¤ì • íŒŒì¼: configs/{settings['config_file']}")
    print(f"ë””ë°”ì´ìŠ¤: {settings['device']}")
    print(f"ë°°ì¹˜ í¬ê¸°: {settings['batch_size']}")
    print(f"ì›Œì»¤ ìˆ˜: {settings['workers']}")
    print(f"AMP: {settings['amp']}")
    print(f"ìºì‹œ: {settings['cache']}")
    print(f"ë¹„ê³ : {settings['notes']}")

    print("\n" + "=" * 60)
    print("            ì‚¬ìš© ë°©ë²•")
    print("=" * 60)
    print(f"cd scripts")
    print(f"python train.py --cfg ../configs/{settings['config_file']}")

    if device_type == 'mps':
        print("\nâš ï¸  Mac ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­:")
        print("  - ì²« ì‹¤í–‰ ì‹œ Metal ì»´íŒŒì¼ë¡œ ì¸í•´ ëŠë¦´ ìˆ˜ ìˆìŒ")
        print("  - ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ batch_sizeë¥¼ 4ë¡œ ì¤„ì´ì„¸ìš”")
        print("  - AMPëŠ” ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

    elif device_type == 'cuda':
        print("\nâœ… GPU ìµœì í™” íŒ:")
        print("  - nvidia-smië¡œ GPU ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§")
        print("  - OOM ì—ëŸ¬ ì‹œ batch_size ê°ì†Œ")
        print("  - ë©€í‹° GPU ì‚¬ìš©: --device 0,1,2,3")

    else:
        print("\nâš ï¸  CPU ì‚¬ìš© ì‹œ ì£¼ì˜ì‚¬í•­:")
        print("  - í•™ìŠµ ì†ë„ê°€ ë§¤ìš° ëŠë¦½ë‹ˆë‹¤")
        print("  - ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê¶Œì¥")
        print("  - ê°€ëŠ¥í•˜ë©´ GPU í™˜ê²½ ì‚¬ìš© ê¶Œì¥")


def create_auto_config(device_type, settings):
    """
    ìë™ ê°ì§€ëœ ì„¤ì •ìœ¼ë¡œ auto_config.yaml ìƒì„±
    """
    config_path = Path('../configs/auto_detected.yaml')

    # ê¸°ë³¸ ì„¤ì • íŒŒì¼ ì½ê¸°
    base_config_path = Path(f'../configs/{settings["config_file"]}')

    if base_config_path.exists():
        import shutil
        shutil.copy(base_config_path, config_path)
        print(f"\nâœ… ìë™ ì„¤ì • íŒŒì¼ ìƒì„±: configs/auto_detected.yaml")
        print("   python train.py --cfg ../configs/auto_detected.yaml")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("\nğŸ” ë””ë°”ì´ìŠ¤ ìë™ ê°ì§€ ì¤‘...")

    # ë””ë°”ì´ìŠ¤ ê°ì§€
    device_type, device_info = detect_device()

    # ê¶Œì¥ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    settings = get_recommended_settings(device_type, device_info)

    # ì •ë³´ ì¶œë ¥
    print_device_info(device_type, device_info, settings)

    # ìë™ ì„¤ì • íŒŒì¼ ìƒì„±
    create_auto_config(device_type, settings)

    return device_type, settings


if __name__ == '__main__':
    device_type, settings = main()